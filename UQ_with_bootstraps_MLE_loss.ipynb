{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69ee8190-fbe5-482f-9ed8-84e69abbe268",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-12 15:40:32.719132: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-03-12 15:40:33.986765: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-03-12 15:40:33.986823: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-03-12 15:40:34.179322: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-12 15:40:34.493038: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a01183d1-378e-4df1-a352-a4289a0436fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Generate random data (10 features, 1 output)\n",
    "n_samples = 1000\n",
    "n_features = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4cf3ae-5af5-46c4-86db-4a6dda9cb7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate random input features (X) and output (y)\n",
    "X = np.random.randn(n_samples, n_features)\n",
    "y = 2 * np.sum(X, axis=1) + np.random.randn(n_samples)  # Simple linear relation + noise\n",
    "\n",
    "# Split the data into training, validation, and test sets (80% train, 10% validation, 10% test)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Step 2: Define a simple fully connected neural network for Phase I (bootstrap models)\n",
    "def create_bootstrap_nn(input_shape):\n",
    "    model = models.Sequential([\n",
    "        layers.InputLayer(input_shape=input_shape),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.Dense(1)  # Output layer for regression\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model\n",
    "\n",
    "# Step 3: Define the noise variance estimation network (Phase II)\n",
    "def create_noise_variance_nn(input_shape):\n",
    "    model = models.Sequential([\n",
    "        layers.InputLayer(input_shape=input_shape),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.Dense(1)  # Output layer for variance prediction\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model\n",
    "\n",
    "# Step 4: Train bootstrap neural networks (Phase I)\n",
    "def train_bootstrap_nns(X_train, y_train, B=10):\n",
    "    bootstrap_models = []\n",
    "    bootstrap_predictions = []\n",
    "\n",
    "    # Generate B bootstrap samples and train models\n",
    "    for _ in range(B):\n",
    "        # Create a bootstrap sample by sampling with replacement\n",
    "        indices = np.random.choice(len(X_train), size=len(X_train), replace=True)\n",
    "        X_bootstrap = X_train[indices]\n",
    "        y_bootstrap = y_train[indices]\n",
    "\n",
    "        # Create and train a new model\n",
    "        model = create_bootstrap_nn(X_train.shape[1:])\n",
    "        model.fit(X_bootstrap, y_bootstrap, epochs=50, batch_size=32, verbose=0)\n",
    "        bootstrap_models.append(model)\n",
    "\n",
    "        # Store predictions on the original training data\n",
    "        predictions = model.predict(X_train)\n",
    "        bootstrap_predictions.append(predictions)\n",
    "\n",
    "    return bootstrap_models, np.array(bootstrap_predictions)\n",
    "\n",
    "# Step 5: Custom likelihood-based cost function (Equation 12)\n",
    "def custom_loss(y_true, y_pred, model_variance):\n",
    "    # r^2(x_i) is already the residual variance computed earlier\n",
    "    r_squared = compute_r_squared(y_true, y_pred, model_variance)\n",
    "    \n",
    "    # Compute the log of the predicted variance\n",
    "    term_1 = tf.math.log(y_pred + 1e-10)  # Add a small epsilon for numerical stability\n",
    "    \n",
    "    # Compute the ratio term\n",
    "    term_2 = r_squared / y_pred\n",
    "    \n",
    "    # Combine both terms for the likelihood-based loss function\n",
    "    loss = 0.5 * tf.reduce_mean(tf.square(term_1 + term_2))\n",
    "    return loss\n",
    "\n",
    "# Compute r^2(x_i) for each bootstrap model\n",
    "def compute_r_squared(y_true, y_pred, model_variance):\n",
    "    residuals = (y_true - y_pred) ** 2 - model_variance\n",
    "    return np.maximum(residuals, 0)\n",
    "\n",
    "# Step 6: Phase II - Train the noise variance estimation network (NNₑ)\n",
    "def train_noise_variance_nn_custom_loss(X_train, y_train, bootstrap_predictions, B=10):\n",
    "    # Calculate r^2(x_i) for each bootstrap model\n",
    "    residuals_all = []\n",
    "    for b in range(B):\n",
    "        residuals = compute_r_squared(y_train, bootstrap_predictions[b], np.var(bootstrap_predictions, axis=0))\n",
    "        residuals_all.append(residuals)\n",
    "    \n",
    "    residuals_all = np.stack(residuals_all, axis=-1)\n",
    "    mean_residuals = np.mean(residuals_all, axis=-1)  # Averaging residuals across bootstrap models\n",
    "\n",
    "    # Create and train the NNₑ for noise variance estimation using the custom loss function\n",
    "    nn_e = create_noise_variance_nn(X_train.shape[1:])\n",
    "    \n",
    "    # Use the custom loss function for training\n",
    "    model_variance = np.var(bootstrap_predictions, axis=0)  # Variance across bootstrap predictions\n",
    "    nn_e.compile(optimizer='adam', loss=lambda y_true, y_pred: custom_loss(y_true, y_pred, model_variance))\n",
    "    \n",
    "    nn_e.fit(X_train, mean_residuals, epochs=50, batch_size=32)\n",
    "    \n",
    "    return nn_e\n",
    "\n",
    "# Step 7: Train the models\n",
    "# Train the bootstrap models (Phase I)\n",
    "bootstrap_models, bootstrap_predictions = train_bootstrap_nns(X_train, y_train, B=10)\n",
    "\n",
    "# Train the noise variance estimation model (Phase II) using the custom likelihood-based loss function\n",
    "nn_e_custom = train_noise_variance_nn_custom_loss(X_train, y_train, bootstrap_predictions, B=10)\n",
    "\n",
    "# Step 8: Evaluate the models on the test set\n",
    "# Make predictions with the bootstrap models\n",
    "bootstrap_preds_test = np.array([model.predict(X_test) for model in bootstrap_models])\n",
    "\n",
    "# Calculate the mean prediction across all bootstrap models\n",
    "bootstrap_mean_preds_test = np.mean(bootstrap_preds_test, axis=0)\n",
    "\n",
    "# Estimate the noise variance using NNₑ\n",
    "predicted_variance_test = nn_e_custom.predict(X_test)\n",
    "\n",
    "# Print results\n",
    "print(f\"Mean prediction for test set: {np.mean(bootstrap_mean_preds_test)}\")\n",
    "print(f\"Estimated noise variance (NNₑ) for test set: {np.mean(predicted_variance_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dbaeb56c-a322-4969-b07b-2dd4d3079c2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 0s 1ms/step\n",
      "25/25 [==============================] - 0s 941us/step\n",
      "25/25 [==============================] - 0s 957us/step\n",
      "25/25 [==============================] - 0s 987us/step\n",
      "25/25 [==============================] - 0s 947us/step\n",
      "25/25 [==============================] - 0s 963us/step\n",
      "25/25 [==============================] - 0s 974us/step\n",
      "25/25 [==============================] - 0s 958us/step\n",
      "25/25 [==============================] - 0s 942us/step\n",
      "25/25 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'y_true' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 115\u001b[0m\n\u001b[1;32m    112\u001b[0m bootstrap_models, bootstrap_predictions \u001b[38;5;241m=\u001b[39m train_bootstrap_nns(X_train, y_train, B\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;66;03m# Train the noise variance estimation model (Phase II) using the custom likelihood-based loss function\u001b[39;00m\n\u001b[0;32m--> 115\u001b[0m nn_e_custom \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_noise_variance_nn_custom_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbootstrap_predictions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mB\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;66;03m# Step 8: Evaluate the models on the test set\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;66;03m# Make predictions with the bootstrap models\u001b[39;00m\n\u001b[1;32m    119\u001b[0m bootstrap_preds_test \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([model\u001b[38;5;241m.\u001b[39mpredict(X_test) \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m bootstrap_models])\n",
      "Cell \u001b[0;32mIn[4], line 104\u001b[0m, in \u001b[0;36mtrain_noise_variance_nn_custom_loss\u001b[0;34m(X_train, y_train, bootstrap_predictions, B)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;66;03m# Use the custom loss function for training\u001b[39;00m\n\u001b[1;32m    103\u001b[0m model_variance \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mvar(bootstrap_predictions, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# Variance across bootstrap predictions\u001b[39;00m\n\u001b[0;32m--> 104\u001b[0m nn_e\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39mcustom_loss(\u001b[43my_true\u001b[49m, y_pred, model_variance))\n\u001b[1;32m    106\u001b[0m nn_e\u001b[38;5;241m.\u001b[39mfit(X_train, mean_residuals, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m)\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m nn_e\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y_true' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Step 1: Generate random data\n",
    "n_samples = 1000\n",
    "n_features = 10\n",
    "\n",
    "# Generate random input features (X) and output (y)\n",
    "X = np.random.randn(n_samples, n_features)\n",
    "y = 2 * np.sum(X, axis=1) + np.random.randn(n_samples)  # Simple linear relation + noise\n",
    "\n",
    "# Split the data into training, validation, and test sets (80% train, 10% validation, 10% test)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Step 2: Define a simple fully connected neural network for Phase I (bootstrap models)\n",
    "def create_bootstrap_nn(input_shape):\n",
    "    model = models.Sequential([\n",
    "        layers.InputLayer(input_shape=input_shape),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.Dense(1)  # Output layer for regression\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model\n",
    "\n",
    "# Step 3: Define the noise variance estimation network (Phase II)\n",
    "def create_noise_variance_nn(input_shape):\n",
    "    model = models.Sequential([\n",
    "        layers.InputLayer(input_shape=input_shape),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.Dense(1)  # Output layer for variance prediction\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model\n",
    "\n",
    "# Step 4: Train bootstrap neural networks (Phase I)\n",
    "def train_bootstrap_nns(X_train, y_train, B=10):\n",
    "    bootstrap_models = []\n",
    "    bootstrap_predictions = []\n",
    "\n",
    "    # Generate B bootstrap samples and train models\n",
    "    for _ in range(B):\n",
    "        # Create a bootstrap sample by sampling with replacement\n",
    "        indices = np.random.choice(len(X_train), size=len(X_train), replace=True)\n",
    "        X_bootstrap = X_train[indices]\n",
    "        y_bootstrap = y_train[indices]\n",
    "\n",
    "        # Create and train a new model\n",
    "        model = create_bootstrap_nn(X_train.shape[1:])\n",
    "        model.fit(X_bootstrap, y_bootstrap, epochs=50, batch_size=32, verbose=0)\n",
    "        bootstrap_models.append(model)\n",
    "\n",
    "        # Store predictions on the original training data\n",
    "        predictions = model.predict(X_train)\n",
    "        bootstrap_predictions.append(predictions)\n",
    "\n",
    "    return bootstrap_models, np.array(bootstrap_predictions)\n",
    "\n",
    "# Custom loss function for likelihood-based cost function (Equation 12)\n",
    "def custom_loss(y_true, y_pred, model_variance):\n",
    "    # r^2(x_i) is already the residual variance computed earlier\n",
    "    residuals = (y_true - y_pred) ** 2 - model_variance\n",
    "    r_squared = tf.maximum(residuals, 0)  # Ensure non-negative residuals\n",
    "    \n",
    "    # Compute the log of the predicted variance (using TensorFlow log)\n",
    "    term_1 = tf.math.log(y_pred + 1e-10)  # Add a small epsilon for numerical stability\n",
    "    \n",
    "    # Compute the ratio term\n",
    "    term_2 = r_squared / y_pred\n",
    "    \n",
    "    # Combine both terms for the likelihood-based loss function\n",
    "    loss = 0.5 * tf.reduce_mean(tf.square(term_1 + term_2))\n",
    "    return loss\n",
    "\n",
    "# Compute r^2(x_i) for each bootstrap model\n",
    "def compute_r_squared(y_true, y_pred, model_variance):\n",
    "    residuals = (y_true - y_pred) ** 2 - model_variance\n",
    "    return np.maximum(residuals, 0)\n",
    "\n",
    "# Step 6: Phase II - Train the noise variance estimation network (NNₑ)\n",
    "def train_noise_variance_nn_custom_loss(X_train, y_train, bootstrap_predictions, B=10):\n",
    "    # Calculate r^2(x_i) for each bootstrap model\n",
    "    residuals_all = []\n",
    "    for b in range(B):\n",
    "        residuals = compute_r_squared(y_train, bootstrap_predictions[b], np.var(bootstrap_predictions, axis=0))\n",
    "        residuals_all.append(residuals)\n",
    "    \n",
    "    residuals_all = np.stack(residuals_all, axis=-1)\n",
    "    mean_residuals = np.mean(residuals_all, axis=-1)  # Averaging residuals across bootstrap models\n",
    "\n",
    "    # Create and train the NNₑ for noise variance estimation using the custom loss function\n",
    "    nn_e = create_noise_variance_nn(X_train.shape[1:])\n",
    "    \n",
    "    # Use the custom loss function for training\n",
    "    model_variance = np.var(bootstrap_predictions, axis=0)  # Variance across bootstrap predictions\n",
    "    nn_e.compile(optimizer='adam', loss=custom_loss(y_true, y_pred, model_variance))\n",
    "    \n",
    "    nn_e.fit(X_train, mean_residuals, epochs=50, batch_size=32)\n",
    "    \n",
    "    return nn_e\n",
    "\n",
    "# Step 7: Train the models\n",
    "# Train the bootstrap models (Phase I)\n",
    "bootstrap_models, bootstrap_predictions = train_bootstrap_nns(X_train, y_train, B=10)\n",
    "\n",
    "# Train the noise variance estimation model (Phase II) using the custom likelihood-based loss function\n",
    "nn_e_custom = train_noise_variance_nn_custom_loss(X_train, y_train, bootstrap_predictions, B=10)\n",
    "\n",
    "# Step 8: Evaluate the models on the test set\n",
    "# Make predictions with the bootstrap models\n",
    "bootstrap_preds_test = np.array([model.predict(X_test) for model in bootstrap_models])\n",
    "\n",
    "# Calculate the mean prediction across all bootstrap models\n",
    "bootstrap_mean_preds_test = np.mean(bootstrap_preds_test, axis=0)\n",
    "\n",
    "# Estimate the noise variance using NNₑ\n",
    "predicted_variance_test = nn_e_custom.predict(X_test)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tfp_for_TN)",
   "language": "python",
   "name": "tfp_for_tn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
