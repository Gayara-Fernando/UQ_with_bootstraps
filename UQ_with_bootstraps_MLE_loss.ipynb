{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69ee8190-fbe5-482f-9ed8-84e69abbe268",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-12 13:53:05.345890: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-03-12 13:53:05.684986: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-03-12 13:53:05.685041: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-03-12 13:53:05.767850: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-12 13:53:05.882376: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a01183d1-378e-4df1-a352-a4289a0436fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Generate random data (10 features, 1 output)\n",
    "n_samples = 1000\n",
    "n_features = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef4cf3ae-5af5-46c4-86db-4a6dda9cb7af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-12 13:55:41.193243: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 31134 MB memory:  -> device: 0, name: Tesla V100S-PCIE-32GB, pci bus id: 0000:06:00.0, compute capability: 7.0\n",
      "2025-03-12 13:55:45.126863: I external/local_xla/xla/service/service.cc:168] XLA service 0x145455bc7f40 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2025-03-12 13:55:45.126910: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100S-PCIE-32GB, Compute Capability 7.0\n",
      "2025-03-12 13:55:45.299287: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2025-03-12 13:55:45.698300: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8907\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1741805746.534773  103987 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 0s 951us/step\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "25/25 [==============================] - 0s 960us/step\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "25/25 [==============================] - 0s 958us/step\n",
      "25/25 [==============================] - 0s 955us/step\n",
      "25/25 [==============================] - 0s 950us/step\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "25/25 [==============================] - 0s 951us/step\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "in user code:\n\n    File \"/common/statsgeneral/gayara/tfp_for_TN/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"/tmp/ipykernel_103804/3634858991.py\", line 90, in None  *\n        lambda y_true, y_pred: custom_loss(y_true, y_pred, model_variance)\n    File \"/tmp/ipykernel_103804/3634858991.py\", line 57, in custom_loss  *\n        r_squared = compute_r_squared(y_true, y_pred, model_variance)\n    File \"/tmp/ipykernel_103804/3634858991.py\", line 71, in compute_r_squared  *\n        residuals = (y_true - y_pred) ** 2 - model_variance\n\n    NotImplementedError: Cannot convert a symbolic tf.Tensor (lambda/pow:0) to a numpy array. This error may indicate that you're trying to pass a Tensor to a NumPy call, which is not supported.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 101\u001b[0m\n\u001b[1;32m     98\u001b[0m bootstrap_models, bootstrap_predictions \u001b[38;5;241m=\u001b[39m train_bootstrap_nns(X_train, y_train, B\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m    100\u001b[0m \u001b[38;5;66;03m# Train the noise variance estimation model (Phase II) using the custom likelihood-based loss function\u001b[39;00m\n\u001b[0;32m--> 101\u001b[0m nn_e_custom \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_noise_variance_nn_custom_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbootstrap_predictions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mB\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;66;03m# Step 8: Evaluate the models on the test set\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;66;03m# Make predictions with the bootstrap models\u001b[39;00m\n\u001b[1;32m    105\u001b[0m bootstrap_preds_test \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([model\u001b[38;5;241m.\u001b[39mpredict(X_test) \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m bootstrap_models])\n",
      "Cell \u001b[0;32mIn[3], line 92\u001b[0m, in \u001b[0;36mtrain_noise_variance_nn_custom_loss\u001b[0;34m(X_train, y_train, bootstrap_predictions, B)\u001b[0m\n\u001b[1;32m     89\u001b[0m model_variance \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mvar(bootstrap_predictions, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# Variance across bootstrap predictions\u001b[39;00m\n\u001b[1;32m     90\u001b[0m nn_e\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m y_true, y_pred: custom_loss(y_true, y_pred, model_variance))\n\u001b[0;32m---> 92\u001b[0m \u001b[43mnn_e\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmean_residuals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m nn_e\n",
      "File \u001b[0;32m/common/statsgeneral/gayara/tfp_for_TN/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filenw2b24ja.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/tmp/__autograph_generated_file45usdo2_.py:6\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.<lambda>\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner_factory\u001b[39m(ag__):\n\u001b[0;32m----> 6\u001b[0m     tf__lam \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m y_true, y_pred: \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwith_function_scope\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlscope\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcustom_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_variance\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlscope\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlscope\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSTD\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tf__lam\n",
      "File \u001b[0;32m/tmp/__autograph_generated_file45usdo2_.py:6\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.<lambda>\u001b[0;34m(lscope)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner_factory\u001b[39m(ag__):\n\u001b[0;32m----> 6\u001b[0m     tf__lam \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m y_true, y_pred: ag__\u001b[38;5;241m.\u001b[39mwith_function_scope(\u001b[38;5;28;01mlambda\u001b[39;00m lscope: \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcustom_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_variance\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlscope\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlscope\u001b[39m\u001b[38;5;124m'\u001b[39m, ag__\u001b[38;5;241m.\u001b[39mSTD)\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tf__lam\n",
      "File \u001b[0;32m/tmp/__autograph_generated_file9pbrjqen.py:10\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__custom_loss\u001b[0;34m(y_true, y_pred, model_variance)\u001b[0m\n\u001b[1;32m      8\u001b[0m do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m      9\u001b[0m retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefinedReturnValue()\n\u001b[0;32m---> 10\u001b[0m r_squared \u001b[38;5;241m=\u001b[39m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcompute_r_squared\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_variance\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m term_1 \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mmath\u001b[38;5;241m.\u001b[39mlog, (ag__\u001b[38;5;241m.\u001b[39mld(y_pred) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1e-10\u001b[39m,), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     12\u001b[0m term_2 \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(r_squared) \u001b[38;5;241m/\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(y_pred)\n",
      "File \u001b[0;32m/tmp/__autograph_generated_file25i1kjdh.py:10\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__compute_r_squared\u001b[0;34m(y_true, y_pred, model_variance)\u001b[0m\n\u001b[1;32m      8\u001b[0m do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m      9\u001b[0m retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefinedReturnValue()\n\u001b[0;32m---> 10\u001b[0m residuals \u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_variance\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     12\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: in user code:\n\n    File \"/common/statsgeneral/gayara/tfp_for_TN/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"/tmp/ipykernel_103804/3634858991.py\", line 90, in None  *\n        lambda y_true, y_pred: custom_loss(y_true, y_pred, model_variance)\n    File \"/tmp/ipykernel_103804/3634858991.py\", line 57, in custom_loss  *\n        r_squared = compute_r_squared(y_true, y_pred, model_variance)\n    File \"/tmp/ipykernel_103804/3634858991.py\", line 71, in compute_r_squared  *\n        residuals = (y_true - y_pred) ** 2 - model_variance\n\n    NotImplementedError: Cannot convert a symbolic tf.Tensor (lambda/pow:0) to a numpy array. This error may indicate that you're trying to pass a Tensor to a NumPy call, which is not supported.\n"
     ]
    }
   ],
   "source": [
    "# Generate random input features (X) and output (y)\n",
    "X = np.random.randn(n_samples, n_features)\n",
    "y = 2 * np.sum(X, axis=1) + np.random.randn(n_samples)  # Simple linear relation + noise\n",
    "\n",
    "# Split the data into training, validation, and test sets (80% train, 10% validation, 10% test)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Step 2: Define a simple fully connected neural network for Phase I (bootstrap models)\n",
    "def create_bootstrap_nn(input_shape):\n",
    "    model = models.Sequential([\n",
    "        layers.InputLayer(input_shape=input_shape),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.Dense(1)  # Output layer for regression\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model\n",
    "\n",
    "# Step 3: Define the noise variance estimation network (Phase II)\n",
    "def create_noise_variance_nn(input_shape):\n",
    "    model = models.Sequential([\n",
    "        layers.InputLayer(input_shape=input_shape),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.Dense(1)  # Output layer for variance prediction\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model\n",
    "\n",
    "# Step 4: Train bootstrap neural networks (Phase I)\n",
    "def train_bootstrap_nns(X_train, y_train, B=10):\n",
    "    bootstrap_models = []\n",
    "    bootstrap_predictions = []\n",
    "\n",
    "    # Generate B bootstrap samples and train models\n",
    "    for _ in range(B):\n",
    "        # Create a bootstrap sample by sampling with replacement\n",
    "        indices = np.random.choice(len(X_train), size=len(X_train), replace=True)\n",
    "        X_bootstrap = X_train[indices]\n",
    "        y_bootstrap = y_train[indices]\n",
    "\n",
    "        # Create and train a new model\n",
    "        model = create_bootstrap_nn(X_train.shape[1:])\n",
    "        model.fit(X_bootstrap, y_bootstrap, epochs=50, batch_size=32, verbose=0)\n",
    "        bootstrap_models.append(model)\n",
    "\n",
    "        # Store predictions on the original training data\n",
    "        predictions = model.predict(X_train)\n",
    "        bootstrap_predictions.append(predictions)\n",
    "\n",
    "    return bootstrap_models, np.array(bootstrap_predictions)\n",
    "\n",
    "# Step 5: Custom likelihood-based cost function (Equation 12)\n",
    "def custom_loss(y_true, y_pred, model_variance):\n",
    "    # r^2(x_i) is already the residual variance computed earlier\n",
    "    r_squared = compute_r_squared(y_true, y_pred, model_variance)\n",
    "    \n",
    "    # Compute the log of the predicted variance\n",
    "    term_1 = tf.math.log(y_pred + 1e-10)  # Add a small epsilon for numerical stability\n",
    "    \n",
    "    # Compute the ratio term\n",
    "    term_2 = r_squared / y_pred\n",
    "    \n",
    "    # Combine both terms for the likelihood-based loss function\n",
    "    loss = 0.5 * tf.reduce_mean(tf.square(term_1 + term_2))\n",
    "    return loss\n",
    "\n",
    "# Compute r^2(x_i) for each bootstrap model\n",
    "def compute_r_squared(y_true, y_pred, model_variance):\n",
    "    residuals = (y_true - y_pred) ** 2 - model_variance\n",
    "    return np.maximum(residuals, 0)\n",
    "\n",
    "# Step 6: Phase II - Train the noise variance estimation network (NNₑ)\n",
    "def train_noise_variance_nn_custom_loss(X_train, y_train, bootstrap_predictions, B=10):\n",
    "    # Calculate r^2(x_i) for each bootstrap model\n",
    "    residuals_all = []\n",
    "    for b in range(B):\n",
    "        residuals = compute_r_squared(y_train, bootstrap_predictions[b], np.var(bootstrap_predictions, axis=0))\n",
    "        residuals_all.append(residuals)\n",
    "    \n",
    "    residuals_all = np.stack(residuals_all, axis=-1)\n",
    "    mean_residuals = np.mean(residuals_all, axis=-1)  # Averaging residuals across bootstrap models\n",
    "\n",
    "    # Create and train the NNₑ for noise variance estimation using the custom loss function\n",
    "    nn_e = create_noise_variance_nn(X_train.shape[1:])\n",
    "    \n",
    "    # Use the custom loss function for training\n",
    "    model_variance = np.var(bootstrap_predictions, axis=0)  # Variance across bootstrap predictions\n",
    "    nn_e.compile(optimizer='adam', loss=lambda y_true, y_pred: custom_loss(y_true, y_pred, model_variance))\n",
    "    \n",
    "    nn_e.fit(X_train, mean_residuals, epochs=50, batch_size=32)\n",
    "    \n",
    "    return nn_e\n",
    "\n",
    "# Step 7: Train the models\n",
    "# Train the bootstrap models (Phase I)\n",
    "bootstrap_models, bootstrap_predictions = train_bootstrap_nns(X_train, y_train, B=10)\n",
    "\n",
    "# Train the noise variance estimation model (Phase II) using the custom likelihood-based loss function\n",
    "nn_e_custom = train_noise_variance_nn_custom_loss(X_train, y_train, bootstrap_predictions, B=10)\n",
    "\n",
    "# Step 8: Evaluate the models on the test set\n",
    "# Make predictions with the bootstrap models\n",
    "bootstrap_preds_test = np.array([model.predict(X_test) for model in bootstrap_models])\n",
    "\n",
    "# Calculate the mean prediction across all bootstrap models\n",
    "bootstrap_mean_preds_test = np.mean(bootstrap_preds_test, axis=0)\n",
    "\n",
    "# Estimate the noise variance using NNₑ\n",
    "predicted_variance_test = nn_e_custom.predict(X_test)\n",
    "\n",
    "# Print results\n",
    "print(f\"Mean prediction for test set: {np.mean(bootstrap_mean_preds_test)}\")\n",
    "print(f\"Estimated noise variance (NNₑ) for test set: {np.mean(predicted_variance_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dbaeb56c-a322-4969-b07b-2dd4d3079c2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 0s 944us/step\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "25/25 [==============================] - 0s 960us/step\n",
      "25/25 [==============================] - 0s 953us/step\n",
      "25/25 [==============================] - 0s 936us/step\n",
      "25/25 [==============================] - 0s 937us/step\n",
      "25/25 [==============================] - 0s 987us/step\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "25/25 [==============================] - 0s 948us/step\n",
      "25/25 [==============================] - 0s 949us/step\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "in user code:\n\n    File \"/common/statsgeneral/gayara/tfp_for_TN/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"/tmp/ipykernel_103804/1361555391.py\", line 104, in None  *\n        lambda y_true, y_pred: custom_loss(y_true, y_pred, model_variance)\n    File \"/tmp/ipykernel_103804/1361555391.py\", line 70, in custom_loss  *\n        residuals = (y_true - y_pred) ** 2 - model_variance\n\n    NotImplementedError: Cannot convert a symbolic tf.Tensor (lambda/pow:0) to a numpy array. This error may indicate that you're trying to pass a Tensor to a NumPy call, which is not supported.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 115\u001b[0m\n\u001b[1;32m    112\u001b[0m bootstrap_models, bootstrap_predictions \u001b[38;5;241m=\u001b[39m train_bootstrap_nns(X_train, y_train, B\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;66;03m# Train the noise variance estimation model (Phase II) using the custom likelihood-based loss function\u001b[39;00m\n\u001b[0;32m--> 115\u001b[0m nn_e_custom \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_noise_variance_nn_custom_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbootstrap_predictions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mB\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;66;03m# Step 8: Evaluate the models on the test set\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;66;03m# Make predictions with the bootstrap models\u001b[39;00m\n\u001b[1;32m    119\u001b[0m bootstrap_preds_test \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([model\u001b[38;5;241m.\u001b[39mpredict(X_test) \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m bootstrap_models])\n",
      "Cell \u001b[0;32mIn[5], line 106\u001b[0m, in \u001b[0;36mtrain_noise_variance_nn_custom_loss\u001b[0;34m(X_train, y_train, bootstrap_predictions, B)\u001b[0m\n\u001b[1;32m    103\u001b[0m model_variance \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mvar(bootstrap_predictions, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# Variance across bootstrap predictions\u001b[39;00m\n\u001b[1;32m    104\u001b[0m nn_e\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m y_true, y_pred: custom_loss(y_true, y_pred, model_variance))\n\u001b[0;32m--> 106\u001b[0m \u001b[43mnn_e\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmean_residuals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m nn_e\n",
      "File \u001b[0;32m/common/statsgeneral/gayara/tfp_for_TN/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filenw2b24ja.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filel4dj_x1t.py:6\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.<lambda>\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner_factory\u001b[39m(ag__):\n\u001b[0;32m----> 6\u001b[0m     tf__lam \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m y_true, y_pred: \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwith_function_scope\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlscope\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcustom_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_variance\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlscope\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlscope\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSTD\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tf__lam\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filel4dj_x1t.py:6\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.<lambda>\u001b[0;34m(lscope)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner_factory\u001b[39m(ag__):\n\u001b[0;32m----> 6\u001b[0m     tf__lam \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m y_true, y_pred: ag__\u001b[38;5;241m.\u001b[39mwith_function_scope(\u001b[38;5;28;01mlambda\u001b[39;00m lscope: \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcustom_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_variance\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlscope\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlscope\u001b[39m\u001b[38;5;124m'\u001b[39m, ag__\u001b[38;5;241m.\u001b[39mSTD)\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tf__lam\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filex_lxaj7e.py:10\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__custom_loss\u001b[0;34m(y_true, y_pred, model_variance)\u001b[0m\n\u001b[1;32m      8\u001b[0m do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m      9\u001b[0m retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefinedReturnValue()\n\u001b[0;32m---> 10\u001b[0m residuals \u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_variance\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m r_squared \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mmaximum, (ag__\u001b[38;5;241m.\u001b[39mld(residuals), \u001b[38;5;241m0\u001b[39m), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     12\u001b[0m term_1 \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mmath\u001b[38;5;241m.\u001b[39mlog, (ag__\u001b[38;5;241m.\u001b[39mld(y_pred) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1e-10\u001b[39m,), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: in user code:\n\n    File \"/common/statsgeneral/gayara/tfp_for_TN/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"/tmp/ipykernel_103804/1361555391.py\", line 104, in None  *\n        lambda y_true, y_pred: custom_loss(y_true, y_pred, model_variance)\n    File \"/tmp/ipykernel_103804/1361555391.py\", line 70, in custom_loss  *\n        residuals = (y_true - y_pred) ** 2 - model_variance\n\n    NotImplementedError: Cannot convert a symbolic tf.Tensor (lambda/pow:0) to a numpy array. This error may indicate that you're trying to pass a Tensor to a NumPy call, which is not supported.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Step 1: Generate random data\n",
    "n_samples = 1000\n",
    "n_features = 10\n",
    "\n",
    "# Generate random input features (X) and output (y)\n",
    "X = np.random.randn(n_samples, n_features)\n",
    "y = 2 * np.sum(X, axis=1) + np.random.randn(n_samples)  # Simple linear relation + noise\n",
    "\n",
    "# Split the data into training, validation, and test sets (80% train, 10% validation, 10% test)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Step 2: Define a simple fully connected neural network for Phase I (bootstrap models)\n",
    "def create_bootstrap_nn(input_shape):\n",
    "    model = models.Sequential([\n",
    "        layers.InputLayer(input_shape=input_shape),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.Dense(1)  # Output layer for regression\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model\n",
    "\n",
    "# Step 3: Define the noise variance estimation network (Phase II)\n",
    "def create_noise_variance_nn(input_shape):\n",
    "    model = models.Sequential([\n",
    "        layers.InputLayer(input_shape=input_shape),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.Dense(1)  # Output layer for variance prediction\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model\n",
    "\n",
    "# Step 4: Train bootstrap neural networks (Phase I)\n",
    "def train_bootstrap_nns(X_train, y_train, B=10):\n",
    "    bootstrap_models = []\n",
    "    bootstrap_predictions = []\n",
    "\n",
    "    # Generate B bootstrap samples and train models\n",
    "    for _ in range(B):\n",
    "        # Create a bootstrap sample by sampling with replacement\n",
    "        indices = np.random.choice(len(X_train), size=len(X_train), replace=True)\n",
    "        X_bootstrap = X_train[indices]\n",
    "        y_bootstrap = y_train[indices]\n",
    "\n",
    "        # Create and train a new model\n",
    "        model = create_bootstrap_nn(X_train.shape[1:])\n",
    "        model.fit(X_bootstrap, y_bootstrap, epochs=50, batch_size=32, verbose=0)\n",
    "        bootstrap_models.append(model)\n",
    "\n",
    "        # Store predictions on the original training data\n",
    "        predictions = model.predict(X_train)\n",
    "        bootstrap_predictions.append(predictions)\n",
    "\n",
    "    return bootstrap_models, np.array(bootstrap_predictions)\n",
    "\n",
    "# Custom loss function for likelihood-based cost function (Equation 12)\n",
    "def custom_loss(y_true, y_pred, model_variance):\n",
    "    # r^2(x_i) is already the residual variance computed earlier\n",
    "    residuals = (y_true - y_pred) ** 2 - model_variance\n",
    "    r_squared = tf.maximum(residuals, 0)  # Ensure non-negative residuals\n",
    "    \n",
    "    # Compute the log of the predicted variance (using TensorFlow log)\n",
    "    term_1 = tf.math.log(y_pred + 1e-10)  # Add a small epsilon for numerical stability\n",
    "    \n",
    "    # Compute the ratio term\n",
    "    term_2 = r_squared / y_pred\n",
    "    \n",
    "    # Combine both terms for the likelihood-based loss function\n",
    "    loss = 0.5 * tf.reduce_mean(tf.square(term_1 + term_2))\n",
    "    return loss\n",
    "\n",
    "# Compute r^2(x_i) for each bootstrap model\n",
    "def compute_r_squared(y_true, y_pred, model_variance):\n",
    "    residuals = (y_true - y_pred) ** 2 - model_variance\n",
    "    return np.maximum(residuals, 0)\n",
    "\n",
    "# Step 6: Phase II - Train the noise variance estimation network (NNₑ)\n",
    "def train_noise_variance_nn_custom_loss(X_train, y_train, bootstrap_predictions, B=10):\n",
    "    # Calculate r^2(x_i) for each bootstrap model\n",
    "    residuals_all = []\n",
    "    for b in range(B):\n",
    "        residuals = compute_r_squared(y_train, bootstrap_predictions[b], np.var(bootstrap_predictions, axis=0))\n",
    "        residuals_all.append(residuals)\n",
    "    \n",
    "    residuals_all = np.stack(residuals_all, axis=-1)\n",
    "    mean_residuals = np.mean(residuals_all, axis=-1)  # Averaging residuals across bootstrap models\n",
    "\n",
    "    # Create and train the NNₑ for noise variance estimation using the custom loss function\n",
    "    nn_e = create_noise_variance_nn(X_train.shape[1:])\n",
    "    \n",
    "    # Use the custom loss function for training\n",
    "    model_variance = np.var(bootstrap_predictions, axis=0)  # Variance across bootstrap predictions\n",
    "    nn_e.compile(optimizer='adam', loss=lambda y_true, y_pred: custom_loss(y_true, y_pred, model_variance))\n",
    "    \n",
    "    nn_e.fit(X_train, mean_residuals, epochs=50, batch_size=32)\n",
    "    \n",
    "    return nn_e\n",
    "\n",
    "# Step 7: Train the models\n",
    "# Train the bootstrap models (Phase I)\n",
    "bootstrap_models, bootstrap_predictions = train_bootstrap_nns(X_train, y_train, B=10)\n",
    "\n",
    "# Train the noise variance estimation model (Phase II) using the custom likelihood-based loss function\n",
    "nn_e_custom = train_noise_variance_nn_custom_loss(X_train, y_train, bootstrap_predictions, B=10)\n",
    "\n",
    "# Step 8: Evaluate the models on the test set\n",
    "# Make predictions with the bootstrap models\n",
    "bootstrap_preds_test = np.array([model.predict(X_test) for model in bootstrap_models])\n",
    "\n",
    "# Calculate the mean prediction across all bootstrap models\n",
    "bootstrap_mean_preds_test = np.mean(bootstrap_preds_test, axis=0)\n",
    "\n",
    "# Estimate the noise variance using NNₑ\n",
    "predicted_variance_test = nn_e_custom.predict(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c8d32c-5238-47ba-a7ed-db4efb066ebc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tfp_for_TN)",
   "language": "python",
   "name": "tfp_for_tn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
